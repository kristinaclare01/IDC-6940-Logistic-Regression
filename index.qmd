---
title: "Predicting disease risk from daily habits: a binary logistic regression analysis"
subtitle: A statistical analysis- Fall 2025
authors: "Kristina Kusem, Shree Basnet, Renan Barbosa (Advisor: Dr. Cohen)"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

<!-- Formatting for background color of entire page -->
<style>
body {
  background-color: lightcyan; /* Change to your desired color */
}
</style>

<!--Formatting for text color: \n 
<span style="color: midnightblue;">This text is blue.</span> And this text is <span style="color: blue;">blue.</span> -->


Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)

<!--::: callout-important
**Remember:** Your goal is to make your audience understand and care
about your findings. By crafting a compelling story, you can effectively
communicate the value of your data science project.

Carefully read this template since it has instructions and tips to
writing!

Nice report!
::: -->


<!-- The introduction should:

-   Develop a storyline that captures attention and maintains interest.

-   Your audience is your peers

-   Clearly state the problem or question you're addressing.



-   Introduce why it is relevant needs.

-   Provide an overview of your approach.

Example of writing including citing references:

*This is an introduction to ..... regression, which is a non-parametric
estimator that estimates the conditional expectation of two variables
which is random. The goal of a kernel regression is to discover the
non-linear relationship between two random variables. To discover the
non-linear relationship, kernel estimator or kernel smoothing is the
main method to estimate the curve for non-parametric statistics. In
kernel estimator, weight function is known as kernel function
[@efr2008]. Cite this paper [@bro2014principal]. The GEE [@wang2014].
The PCA [@daffertshofer2004pca]*. Topology can be used in machine
learning [@adams2021topology]

For Symbolic Regression [@wang2019symbolic] *This is my work and I want
to add more work...*

Cite new paper [@su2012linear]  -->

## Outline/ draft of Introduction and Literature Review:
<p><b>Please note that this is a general outline/ rough draft of the literature review containing all article summaries from everyone's individual research. All research summaries will be revised and edited further, and a polished version of this literature review will be posted next week, 10/6/25. </b></p>

### Introduction:

-State what logistic regression is and why it is commonly used in research (it helps make predictions about categorical outcome variables, usually a binary outcome. It also helps to analyze several predictors simultaneously and how they all relate to the outcome variable at once). Discuss how we usually model LR and how we interpret results.<br>
-Discuss which types of scenarios logistic regression is usually used in. It is actually used in a variety of contexts and is very useful for solving everyday problems around the world:<br>
 -disease detection- identifying predictors of the presence of a disease helps people make preventive decisions regarding their health. It also helps practitioners more easily identify and diagnose specific health conditions. <br>
-predictions about the way people make decisions; specifically, healthcare decisions<br>
-traffic and accident prediction <br>
-fraud detection<br>

### Literature Review:

#### Articles about interesting methodologies involving logistic regression:

<p><b>1. "From Logistic Regression to the Perceptron Algorithm: Exploring Gradient Descent with Large Step Sizes"</b></p>
https://huggingface.co/papers/2412.08424

<p><small><small>The author presents some interesting findings that by connecting Logistic regression with gradient descent there is a link with the perceptron algorithm. With really large steps it acts like a perceptron which in some sense links it back to the Deep Equilibrium networks study.
This paper is interesting because it is counter intuitive and brings a lot of things to reflect about classification and optimization theory.</small></small></p>

<p><b>2. "Large Language Model Confidence Estimation via Black-Box Access"</b></p>
https://huggingface.co/papers/2406.04370

<p><small><small>This paper addresses the problem of estimating the confidence of large language model (LLM) outputs when only black-box (query-only) access is available. It is a simple technique that uses Logistic Regression to classify and validate the confidence of the outputs. The problem of using the black-box models is that there is no control over the model itself, but in some cases the benefits and the value of buying these services that provide a black-box model outweighs training your own custom so this is a framework that attempts to overcome the challenges.</small></small></p>

<p><b>3. "BERT or FastText? A Comparative Analysis of Contextual as well as Non-Contextual Embeddings"</b></p>
https://huggingface.co/papers/2411.17661

<p><small><small>My personal opinion:
This research doesn't explicitly state why Logistic Regression is important, but it did use it as the classifier for all of the experiments to maintain methodological simplicity. All embeddings were passed to a multinomial logistic regression (MLR) classifier for classification into target labels. Which shows the versatility of logistic regression when elaborating an experiment to test a hypothesis.

The main goal of the paper is to analyze the effectiveness of non-contextual embeddings from BERT models (MuRIL and MahaBERT) and FastText models (IndicFT and MahaFT) for NLP tasks. The authors compare these embeddings to contextual and compressed variants of BERT  aiming to fill a research gap, because previous research did not explore non-contextual embeddings.

The research is important because it addresses the challenges faced by NLP in
low-resource languages (The ones that lack big annotated datasets to properly train). The selection of an effective embedding method is extremely important for strong NLP performance. The research tries a promising alternative, non-contextual BERT embeddings, which can be obtained through a simple table lookup, unlike contextual embeddings that require a full forward pass through the model. This is particularly relevant for getting model performance with much better computational efficiency.

The methodology is quite interesting. For the FastText, which is a non-contextual embedding by default, they had to create a custom vocabulary. Which was achieved by concatenating the training and validation datasets and then passing them through a text vectorizer, which generated vectors for every word in the dataset. The vectorizer returned the vocabulary as a list of words in decreasing order of their frequency. Then the FastText model was then loaded using the FastText library, and for each word in the vocabulary, a word vector was retrieved to construct the embedding matrix. For each sentence, the text was split into individual words, and the corresponding embeddings were retrieved from the embedding matrix. These embeddings were then averaged to produce the final sentence embeddings. 

Furthermore they did not stop with FastText, they also experimented with compressed embeddings by reducing the dimensionality from 768 (the traditional BERT embedding dimension) to 300. This compression was performed using Singular Value Decomposition (SVD) to select the most relevant features, extracting the top 300 components for all the combinations of contextual as well as non-contextual for MahaBERT as well as Muril. 

In this approach it's interesting how they did use Logistic regression for simplicity. All embeddings were then passed to a multiple logistic regression(MLR) classifier for classification into target labels.

I understood that as a result they did show that contextual BERT embeddings perform better than non-contextual ones, including both non-contextual BERT embeddings and FastText. 
So in the end they proved that their approach did not improve much or provided much resource to support this different approach.
They also showed that when non-contextual BERT embeddings are compressed, their performance drops, and FastText performs better than compressed noncontextual BERT. But this is a questionable finding.

The limitations of the research is that even in most cases it was apparent that compression lowers the performance of non-contextual BERT embeddings. The effect of compression on contextual embeddings varies across datasets and there is no consistent way to properly derive conclusions.</small></small></p>



<p><b> 4. "Incorporating LLM Priors into Tabular Learners"</b></p>
https://huggingface.co/papers/2311.11628

<p><small><small>There have been implementations of transformer based architectures for tabular data. Most of the time it has been utilized for generating synthetic data for likelihood free models or for cases where there is not enough data for fitting a model.

The goal of this research was to bootstrap a way so one could use off the shelf models like Chatgpt which are really good at generalization to perform similarly to dedicated models trained on tabular data such as tabLLM. This is important because it is fairly cheaper and more accessible than training a model from scratch and it overcomes the complexities of developing a specialized encoder.

The methodology is a pretty hacky solution where they serialized the tabular data so they could prompt the models are are just trying to obtain back a categorization through prompt engineering which will be attributed a value which is manually tuned by the authors and this value is later used on the Monotonic Logistic Regression.

The limitations are quite clear. There is no way to guarantee the black box model output will be consistent. You have to manually categorize and do some prompt engineering. The model has bias so it either works really well or it doesn't.

The bright side is that this approach is extremely cheap and is accessible. It can be used to test ideas and hypothesis as well rapidly prototype before committing to a more definite solution such as tabLLM.</small></small></p>

<p><b>5. "Using a monotonic density ratio model to increase the power of the goodness-of-fit test for logistic regression models with case-control data"</b></p>
https://onlinelibrary.wiley.com/doi/10.1002/sim.10183?af=R

<p><small><small>Case-control sampling is used because it is a quick, economical, and efficient method for studying rare diseases or outcomes, long latent periods, or outbreaks. It allows researchers to investigate multiple potential risk factors simultaneously for a single outcome and is especially useful when prospective cohort studies are not feasible.

The author's goal seems to be to improve the statistical power of the goodness-of-fit test for logistic regression models when used with case-control data. They improved upon a previous popular method from Qin and Zhang, instead of using the nonparametric empirical distribution function, we use the constrained nonparametric MLE of G(x) to further improve the power performance of the Kolmogorov-Smirnov-type goodness-of-fit test for logistic models.

Before drawing conclusions from a logistic regression model, it's crucial to verify that the model's assumptions hold true for the data and there are many limitations. Case Study data is specially complicated because there is not enough data and there are too many unknowns.

The authors spare no comments on the limitations, the bigger limitations are:
●	The test is designed for goodness-of-fit and cannot be used to compare two different logistic regression models
●	The test has no power when the only covariate is categorical. In this situation, the logistic model is "saturated," meaning it perfectly fits the data by definition and cannot be misspecified.

Their results are overall quite interesting as they demonstrated that they could bootstrap an algorithm that is quite clever and intuitively it shouldn't work. It is a hard problem to solve so it is interesting out of the box thinking
</small></small></p>




#### Articles about using logistic regression in medical contexts:

<p><b>1. "Understanding logistic regression analysis" </b>
<small><small>This article discusses the usefulness of a logistic regression model and describes how to set up and interpret such a model when analyzing data. A logistic regression model is especially useful when dealing with more than two predictors and an outcome variable that has two or more classes or categories. This article gives an example of a logistic regression analysis with a synthetic dataset about patients undergoing a drug treatment. The outcome variable is categorical and binary in nature, taking on values of survived (1), or did not survive (0). The result of the analysis gives an explanation of how to interpret output from the model. One must take the expnentials of the slopes in the model to find the chances (the probability) of an event occurring. It is noted that in order to correctly understand results of a logistic regression, one must carefully consider the differences between the odds ratio, the log odds, and the probabilities of events occurring. Another important point in the article states the importance of feature selection; a common way that predictors are selected for a logistic regression model is through a preliminary univariate analysis. During data preprocessing, all predictors are analyzed individually in relation to the outcome variable in a univariate analysis, and all significant predictors are used in the multivariate logistic regression analysis.  </small></small></p>

<p><b>2. "Determinants of coexistence of undernutrition and anemia among under- five children in Rwanda; evidence from 2019/20 demographic health survey: Application of bivariate binary logistic regression model"</b></p>

<p><small><small>-This article details a bivariate binary logistic regression model. There are two outcome variables: anemia and undernutrition in children under five years of age in Rwanda. The study analyzes the relationship between the two outcome variables, as well as the relationship between 26 predictors relating to the childrens’ preexisting health conditions, family information, details about the parents, and relevant geographic information. One result of the study was that the relationship between the two outcome variables, presence of malnutrition and presence of anemia, was found to be significant. It was also found that six predictors were found to be sinificant: mother’s age, drinking water, other children in household, child gender, birth order, and gender of head of household. The conclusion states that improving maternal education, supplementing with vitamin A and other nutrient dense foods, providing a healthy home environment environment, and decreasing maternal anemia may help improve rates of malnutrition and anemia in children.</small></small></p>

<p><b>3. "Predictors of hospital admission when presenting with acute on chronic breathlessness: Binary logistic regression" </b></p>

<p><small><small>In this study, emergency room data from one hospital is analyzed to determine common predictors of patients that are admited to the hospital. Specifically, patients presenting to the emergency room with acute on chronic breahtlessness were surveyed to collect data that would help researchers understand common factors among those admitted to the hospital. Knowing common predictors ahead of time helps hospital staff more easily identify pateints who are more at risk for being admitted to the hospital, and also helps identify which patients would be more likely to be able to be discharged without being admitted. A binary logistic regression analysis of the data revealed that the odds of admission to the hospital were positively correlated with age, talking to a doctor about symptoms, and the presence of preexisting heart conditions; however, the odds of being admitted to the hospital were negatively associated with blood oxygen levels. </small></small></p>



<p><b>4. "Risk factors for airplane headache: A multivariate logistic regression analysis in a population of career flight personnel"</b></p>

<p><small><small>-A new medical condition was recently recognized in 2004: airplane headache (AH), a condition described as a headache induced while while taking off or landing in an airplane. Because AH is a relatively new addition to medical dictionaries, it is an underexplored condition that requires additional research. This study saught to identify common risk factors significantly associated with airplane headache to aid both travelers and airline employees. Two binary logistic regression models were constructed to compare two groups against the airplane headache group. The first binary logistic regression compared the airplane headache group to the no headache group, and 10 significant predictors of AH were identified; this model’s predictive power was found to be very high. The second binary logistic regression model compared the airplane headache group to a group labelled “other headache” (individuals with symptoms of other types of headaches). The result from this analysis showed four significant predictors; however, the predictive power of the model was shown to be very low. To conclude, it can be stated that binary logistic regression is a very effective way to find strong predictors of airplane headache when compared to those who do not have any headaches while flying.</small></small></p>




<p><b>5. "Exploring the medical decision- making patterns and influencing factors among the general Chinese public: a binary logistic regression analysis"</b></p>
<p><small><small><small>
Problem: 
-Researchers are seeking to understand top driving factors behind decisions made about healthcare and medical issues. The population of interest is the general Chinese public.Previous research in this field has identified two main types of medical decision making: unilateral and collaborative decision making.Unilateral decision making means there is one main entity making the medical decision, such as a single patient, a patient’s family, or a doctor. Previous research shows that patient families have a very strong influence over a patient’s medical decisions.Collaborative decision making means there are two or more parties involved in the decision making process. Three subgroups are defined: doctor group, doctor- patient group, patient- family group, and patient-doctor-family group.There is a lack in research in this field. More needs to be known about factors that play a role in medical decision making processes. This study’s results will be generalizable to China as well as nations around the world. 

Solution:
-Use binary logistic regression to classify points into two categories: unilateral decision making (value of 1), or collaborative decision making (value of 0)
-This model is ideal because it takes into account variable interactions. Also used often in the medical field
-The equation of the model is given. It is in the form of the log odds of the desired event happening.

Data:
-2696 data points with attributes including age, education, occupation, family situation, religion, economic status and medical payment methods
-Data was collected via survey and included only residents of China from 31 provinces
-The data was gathered by the researchers that wrote this study
-A power analysis was conducted to determine how many data points would be needed in order to have reliable results after statistical analysis. A G-power test showed that only 2040 valid data points were needed

Results:
-Survey results showed that 30% of responses were categorized as unilateral decision making, while 70% were categorized as collaborative decision making. The top category of unilateral decision making was doctor- led decisions, while the top category for collaborative decision making was patient- doctor- family decisions.
-Significant predictors were identified with p-values less than 0.05. Significant predictors of unilateral decision making were gender, education level, family status, and religious beliefs. Different occupations also significantly predicted unilateral decision making. 
-Odds ratios are given for some predictors, with researchers stating that certain categories of specific predictors are x.xx times more likely than the reference group to be a unilateral decision maker.
-The significance of the regression model’s intercept is interpreted, and it is significant. This means when all variables are at their reference levels, there is a low likelihood of the outcome variable taking on a value of a unilateral decision making process.
-The goodness of fit test used in this study is McFadden’s R-squared value. The value was0.065, and researchers state that this value shows a good fit of the model. It is explained that R squared values for studies in the social sciences are rarely ever close to a perfect fit. 

Conclusions:
-The researchers discuss why there are contrasting results from this study versus studies in Western countries. They identify different cultural values in different geographic regions, which ultimately lead to different medical decision making processes. 
-Results are discussed more in depth, with researchers attempting to identify causes behind the correlations that were identified. 

Limitations:
-The study’s data is solely from China, so results may not be generalizable to global populations.
-The binary logistic regression model may not be complex enough to account for the complexities of all the predictors involved in healthcare decision making. Researchers suggest using more complex models in future research. 
</small></small></small></p>


#### Articles about using logistic regression in traffic and engineering contexts:

<p><b>1. "Binary logistic regression analysis of factors affecting urban road traffic safety"</b></p>

<p><small><small>Problem:
The introduction features a literature review establishing relevance of the topic. Several studies about traffic accidents are discussed and cited. Traffic accidents are becoming more common as traffic increases due to population increases. Researchers aim to find which factors in traffic are more closely associated with the occurrence of traffic accidents. 

Solution:
The researchers use a binary logistic regression model to study which factors are more correlated to the occurrence of traffic accidents. The advantages of a binary logistic regression are discussed. Logistic regression allows researchers to make predictions about probability of a dependent variable being sorted into a certain class. Logistic regression also allows for the researchers to determine which predictors more significantly impact the outcome variable. 
To set up the study, researchers defined the dependent variable, y, as a binary outcome of either no accident (value of 0) or presence of an accident (value of 1). The independent variables are defined as 25 factors that are grouped under four categories consisting of environmental factors, driver attributes, road attributes, and vehicle factors. Before analyzing the data, it was preprocessed to eliminate outliers, normalize all predictor values to similar scales, eliminate redundancy, Then, the predictors were run through a multicollinearity test to determine if any needed to be excluded from the model; none of the factors showed significant multicollinearity and all 25 were kept in the model. The calculation used for collinearity involved the correlation coefficient, R, tolerance (T), and variance inflation factor (V).

Results:
A binary logistic regression model was fitted to the data, and it was found that the model fit well. To assess the goodness of fit, the determination coefficient, R^2, value was calculated. 
The strongest predictors of a traffic incident were found to be driver behavior, weather, road conditions, and lighting. 

Limitations:
The paper states that research in this area can be improved by using real- time data about weather, road conditions, and driver status. Using data as it occurs in real time may help make better predictions about traffic safety risks.

Dataset:
The original data was sourced from the International Transport Forum with 5350 datapoints. After data preprocessing, the data was reduced to 3500 data points. </small></small></p>




<p><b>2. "Identification of factors influencing severity of motorcycle crashes in Dhaka,
Bangladesh using binary logistic regression model"</b></p>
<p><small><small><small>

Problem:
-One of the most common unnatural causes of death across the world is road accidents,
so it is important to identify strong predictors associated with such accidents.
-According to the World Health Organization, most of the road crash deaths that occur worldwide happen in developing countries. 
-Researchers focus on motorcycle crashes in Dhaka, the capital city of Bangladesh. It is stated that the rate of road crashes in Bangladesh is significantly greater than other
developing countries, and Dhaka has the greatest amount of motorists and
reported motorcycle crashes.
-It is noted that most research about this topic is done on data from developed countries
and not developing countries. So the conclusions drawn from existing studies may not
be applicable to the problems the developing countries are facing
-Knowing which predictors are strongly associated with motorcycle crashes in Dhaka helps builders and developers eliminate or reduce these risk factors as they are building new roads.
This study serves as a step in preventing more motorcycle road crashes as developing countries are being built. 

Solution:
-Researchers conduct a binary logistic regression analysis to identify predictors most strongly associated with the occurrence of the outcome, motorcycle crash severity (fatal/ non- fatal)
-They began the study by choosing predictors identified in previous literature about similar topics. Commonly identified predictors of motorcycle crashes are grouped into five broad categories: environment, road characteristics, driver characteristics, motorcycle features, and type of collision.
-The binary logistic regression equation is given as the log odds of the probability of the occurrence of the outcome. An explanation of every variable in the equation is given (slopes, intercept, odds ratio, and relation to the outcome variable).

Data:
-Data was collected from 2006 to 2015 from the Accident Research Institute of Bangladesh University of Engineering and Technology. Only 316 data points were used, and each contained information about motorcycle crashes. 
-There are five broad categories encompassing all predictors. The five categories and predictors are:
1. Environmental factors- date/time, lighting, weather
2. Collision type- five different types of collisions
3. Driver characteristics- gender, age, alcohol consumption, and use of a helmet
4. Road characteristics-location, traffic characteristics, road conditions
5. vehicle characteristics- type of other vehicle in crash, weight of other vehicle in crash, motorcycle condition, and more 
-The outcome variable, crash injury severity, originally had four levels. Observations from this predictor were then reclassified as either “fatal” or “non-fatal” for a binary outcome.
-To determine which predictors to include in the dataset, researchers conducted a univariate analysis and a chi- square test of each individual predictor to assess significance. All significant predicators were then chosen for the dataset, used for the binary logistic regression. After this, multicollinearity was assessed using the VIF and none of the predictors showed multicollinearity.

Results/ Conclusions:
-After conducting the binary logistic regression, 11 out of the 16 included predictors were found to be significantly associated with the outcome. The significant predictors were day of the week, seasonal condition, time of day, three types of road characteristics, crash type, condition of motorcycle, type of other vehicle in accident, use of helmet, and alcohol consumption
-The regression curve from the analysis was also found to be significant
-Goodness of fit was assessed using a test called the Hosmer and Lemeshow test
-The discussion section details each significant predictor, and interpretations of slopes are given in relation to the outcome variable and the reference groups. 
-Some of the findings were consistent with other studies, and some of the findings contradict conclusions in previous studies.
-Most notable conclusions from this study that researchers believe would improve road safety and reduce motorcycle accident severity in developing countries:
1. better lighting conditions for enhanced visibility
2. solution to wet/ slippery roads during rainy season
3. educate drivers about how to drive during unsafe conditions, such as nighttime, heavy rain, and heavy traffic on weekends. Also educate drivers to follow proper safety and speeding regulations. And better education/ training/ evaluation for drivers operating larger vehicles 
4. improved pedestrian walkways and road areas
5. strict enforcement of laws regarding helmet use and alcohol while driving
Limitations:
-Missing information in the data: Some important predictors were entirely excluded from the study due to missing information in the dataset. So there may be some extremely relevant predictors that have yet to be studied. There is also an ongoing issue of accidents that go unreported due to lack of fatality, and drivers do not report these incident. </small></small></small></p>



#### Articles about using logistic regression in environmental issues:

<p><b>"Priority prediction of Asian Hornet sighting report using machine learning methods"</b></p>
https://huggingface.co/papers/2107.05465

-The goal of the research is to create an automated system to predict the priority of Asian giant hornet sighting reports. Asian giant hornets are an invasive species that poses a significant threat to native bee populations and local beekeeping, as well as to public safety due to their aggressive nature and potent venom. So it's very important that reports are properly assessed for priority.

-The authors did model the priority prediction of sighting reports as a two-classification problem. This approach was pretty clever and simple. The goal was to just classify reports as either a "true positive" or a "false positive".

-Their methodology is a straightforward application of logistic regression with feature extraction. They came to realize that they needed Location Feature, Time Feature, Image Feature, Text Feature.

-Location Feature considers the probability of a hornet being observed at a specific location based on known hornet migration patterns and habits.
Time Feature accounts for the hornet's seasonal behavior. Since hornets are most active from April to December, a report submitted during this period is more likely to be positive.
Image Feature is the number of images attached to a report and they came to notice that it is correlated with the increase of its credibility. 
Text Feature is the textual description's length and keywords. A longer text is considered more credible because it contains more evidence. The model also uses a specific dictionary of hornet characteristics to identify relevant keywords.

-They then used a weighted binary cross-entropy function and the logistic regression is just mapping the probability given the feature vector.

-The model achieved an average prediction accuracy of 83.5% on positive reports with the best weighting parameter settings, but still far from other works which achieved about 93% using Deep Learning. So this is the main limitation, still needs a lot of improvement or maybe it will never outmatch other methods due to hidden limitations.

-My opinion on this paper is that the Logistic Regression has interesting properties, after all it is a generalized linear model, which conducts mapping from any real number to probability values.





#### Articles about detecting fraud:

<p><b>1.	"Using Binary logistic Regression to Detect Health Insurance Fraud"</b></p>

Problem: 
-Insurance fraud in health insurance industry. Specifically, patients treated at private clinics or hospitals. 
-Intro of the article explains why this topic is relevant. Between 1965 and 2008, the cost of healthcare increased significantly, and as a result, health insurance fraud has increased. There needs to be effective tools at detecting this fraud.
-If fraud is decreased, it will help the economy as a whole, it will help insurance companies, and it will lower premium payments made by customers. 
-Fraud is committed by three types of entities: consumer, provider, and payer fraud.
-Literature review cites common predictors of health insurance fraud: diagnoses, service cost, number of claims from individual, greatest costing claim, probability of anomaly, excessive charges by care facilities, and more.





Data:
-Original dataset contained 26 independent variables
-Data was collected from a time span of January 2022 through November 2022
-about 123,000 data points with no missing values.
-The predictors are of varying types, including numerical, categorical, and binary values
-The dependent variable is fraud, with a value of 1 for fraud present, or 0 for no fraud present.

Solution to Problem:
Why they selected this model:
-Building a binary logistic regression model to detect health insurance fraud
-logistic regression is selected as the analytic technique because they want to assess effects of categorical variables on a categorical dependent variable. They also cite that logistic regression is the most accurate type of regression model with the kind of classification they are performing in this study.
-Fraud detection commonly employs binary prediction models
-The logistic regression model also provides estimates between 0 and 1, which help investigators estimate probability of fraud
-Researchers provide the equation they use to calculate the log odds of the event of interest (occurrence of fraud)
The method:
-They calculate likelihood of an individual committing fraud
-They calculate total cost accrued by an individual. Then they perform the logistic regression using this calculation
-The model works by identifying outliers and classifies them as potential fraudulent activity
-Before testing the model, researchers hypothesize that there will be a positive relationship between overall cost accrued by patient and likelihood of fraud. Costs include doctor visit costs, prescription drug costs, lab costs, costs of medical symptoms, and total cost of expensive prescriptions. 
-When running the model, none of the coefficients were zero, which means that there exists a significant relationship between the outcome and the predictor variables. 
-Predictors were tested for multicollinearity before the model was run. Pearson correlation coefficients were obtained, and any predictors with a Pearson value of greater than 0.8 were excluded from the model. Only eight predictors remained after removing the predictors that were strongly correlated.
-Different models were constructed using only the most important predictors. When taking away the least important predictor, the log likelihood was calculated to assess the accuracy of the model. The best performing model contained six of the original predictors. 

Results:
-Six predictors were found to be significant in predicting health insurance fraud. The predictors are office visit cost, prescription costs, lab costs, symptom cost, and two expensive prescription drug costs. 
-There is a thorough interpretation of model slopes. For example, “the likelihood of fraud increases by .005188 for every unit increase in pharmacy cost.” Interpretations for the most significant predictors are given in this way. 
-A Chi- Suare test for independence was used to determine whether at least one predictor was significantly related to the outcome. It was concluded that at least one of the six predictors was significant. 
-The model was found to be about 99% accurate when predicting no fraud, but only about 76% accurate when predicting fraud. 
-An example is included that shows how to calculate the probability that an individual will commit insurance fraud, given values for the six predictors in the equation. 
-The study concludes by stating the importance and relevance of continuing to develop new fraud detection models. 

Limitations:
-No limitations are explicitly stated in this paper. However, it can be considered a limitation that only data from middle eastern countries was used in the study. To make the results of the study more generalizable, data from other regions of the world should be included in a more comprehensive study.



<!--## Methods

-   Detail the models or algorithms used.

-   Justify your choices based on the problem and data.

*The common non-parametric regression model is*
$Y_i = m(X_i) + \varepsilon_i$*, where* $Y_i$ *can be defined as the sum
of the regression function value* $m(x)$ *for* $X_i$*. Here* $m(x)$ *is
unknown and* $\varepsilon_i$ *some errors. With the help of this
definition, we can create the estimation for local averaging i.e.*
$m(x)$ *can be estimated with the product of* $Y_i$ *average and* $X_i$
*is near to* $x$*. In other words, this means that we are discovering
the line through the data points with the help of surrounding data
points. The estimation formula is printed below [@R-base]:*

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$$W_n(x)$ *is the sum of weights that belongs to all real numbers.
Weights are positive numbers and small if* $X_i$ *is far from* $x$*.*

*Another equation:*

$$
y_i = \beta_0 + \beta_1 X_1 +\varepsilon_i
$$

## Analysis and Results

### Data Exploration and Visualization

-   Describe your data sources and collection process.

-   Present initial findings and insights through visualizations.

-   Highlight unexpected patterns or anomalies.

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```



```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Modeling and Results

-   Explain your data preprocessing and cleaning steps.

-   Present your key findings in a clear and concise manner.

-   Use visuals to support your claims.

-   **Tell a story about what the data reveals.**

```{r}

```

### Conclusion

-   Summarize your key findings.

-   Discuss the implications of your results.

## References

-->
